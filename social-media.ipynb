{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f0f2200-a90d-4e40-8b03-a4ec9a383397",
   "metadata": {},
   "source": [
    "<center><h1>Social Media Sentiment Analysis</h1>\n",
    "    <h2>by Rebecca Hinrichs</h2>\n",
    "    <h3>SUMMER 2023</h3></center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18450b23-26a1-4d60-8ed8-bb45a8a9a78b",
   "metadata": {},
   "source": [
    "<b>Purpose:</b> The purpose of this section is to test concepts of hyperparameter optimization.\n",
    "\n",
    "<b>Data:</b> We were provided with a folder `yelp_review_polarity_csv`, consisting of files `train_small.csv` and `test_small.csv` containing pre-split training/testing groups of data. First column is class label (\"1\" and \"2\" in this example), second column is text. Note that entries are within double quotes (\"). These files will be loaded using python package 'pandas' and stored in pandas dataframes (like 'data frame' in R). Each column in pandas data frame is a dictionary, column name being the key. \n",
    "<br><br>\n",
    "Additionally, we were provided with a folder `glove.6B` containing a tokenizer dictionary `glove.6B.100d.txt`, which is publicly-available <a href=\"https://nlp.stanford.edu/projects/glove/\">here</a>. The dictionary includes a vocabulary of 400k words in 100 dimensions. \n",
    "\n",
    "<b>Approach:</b> We will perform hyperparameter optimization using Python's `HyperOpt` library on a dual-layer LSTM Model, optimizing at least 2 tuning parameters.\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a628ca22-3e5f-4d8c-a5d8-6be573cbce69",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "<center><h2>Data Preparation</h2></center>\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b78089-449b-4180-8578-3d549b046c31",
   "metadata": {},
   "source": [
    "#### Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61c22b93-9371-4968-bdec-100c9a0441cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Text Samples :: 22000\n",
      "\n",
      "Dimensions of Training Data :: (20000,)\n",
      "Dimensions of Testing Data :: (2000,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Collect the text data from the provided directory\n",
    "import pandas as pd\n",
    "# Load file directory\n",
    "directory = 'yelp_review_polarity_csv/'\n",
    "train_data = pd.read_csv(str(directory)+'train_small.csv',\n",
    "                         sep=',', names=['class','text'], on_bad_lines='skip')\n",
    "test_data = pd.read_csv(str(directory)+'test_small.csv',\n",
    "                        sep=',', names=['class','text'], on_bad_lines='skip')\n",
    "\n",
    "# Report data shapes\n",
    "print('\\nTotal Text Samples :: ' + str(len(train_data)+len(test_data)))\n",
    "print('\\nDimensions of Training Data :: ' + str(train_data['class'].shape))\n",
    "print('Dimensions of Testing Data :: ' + str(test_data['class'].shape) +'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b383bab-826b-402e-a2d3-2c6ad961d33b",
   "metadata": {},
   "source": [
    "#### Single Series Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16d20ed0-6608-4dc2-8c26-4db0b9adc4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Text :: \n",
      " Food is tasty; however, food establishment REFUSED to honor purchased coupon from Living Social.  DO NOT spend your hard-earned money at India Palace.\n",
      "\n",
      "Class of Sample Text ::  1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display a sample of a text from the training data\n",
    "import numpy as np\n",
    "pick_me = np.random.randint(0,len(train_data)) # pick a random line of text\n",
    "\n",
    "# Display a printout of the sample\n",
    "print('\\nSample Text :: \\n', train_data['text'][pick_me])\n",
    "print('\\nClass of Sample Text :: ', train_data['class'][pick_me], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fedfd0-9c78-496f-a2dc-2a2595722b17",
   "metadata": {},
   "source": [
    "<center><h2><u>Data Pre-Processing</u></h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea4fe57-9f5c-4ec0-a863-aaad80f1fa09",
   "metadata": {},
   "source": [
    "#### Cleanse the input space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49da38a7-504f-4a17-942d-534807aa01b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BEFORE FILTERING ::\n",
      "Dimensions of Training Data :: (20000,)\n",
      "Dimensions of Testing Data :: (2000,)\n",
      "\n",
      "AFTER FILTERING ::\n",
      "Dimensions of Training Data :: (20000,)\n",
      "Dimensions of Testing Data :: (2000,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cleanse text of any missing or corrupt data\n",
    "print('\\nBEFORE FILTERING ::')\n",
    "print('Dimensions of Training Data :: ' + str(train_data['class'].shape))\n",
    "print('Dimensions of Testing Data :: ' + str(test_data['class'].shape))\n",
    "\n",
    "# Remove rows with missing data\n",
    "train_data = train_data.dropna()\n",
    "test_data = test_data.dropna()\n",
    "\n",
    "# Remove rows with no comments\n",
    "train_data = train_data[train_data.text.apply(lambda x: x !=\"\")]\n",
    "test_data = test_data[test_data.text.apply(lambda x: x !=\"\")]\n",
    "print('\\nAFTER FILTERING ::')\n",
    "print('Dimensions of Training Data :: ' + str(train_data['class'].shape))\n",
    "print('Dimensions of Testing Data :: ' + str(test_data['class'].shape) +'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3ba3d0-dcc9-43d5-a1c0-2acd513b3830",
   "metadata": {},
   "source": [
    "<u><center><h2>Dataâ†’Tensor Transformation</h2></center></u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b8510d-f888-4393-b2d7-305011544fdf",
   "metadata": {},
   "source": [
    "#### Vectorize the inputs (x-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd8ad963-6106-45f9-ab4b-43f052b69b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 47081 unique tokens\n",
      "Number of Tokens used :: 20000\n",
      "Longest Text Sample :: 1317\n",
      "\n",
      "Imported Sample Text ::\n",
      " Food is tasty; however, food establishment REFUSED to honor purchased coupon from Living Social.  DO NOT spend your hard-earned money at India Palace.\n",
      "\n",
      "Tokenized Sample Text ::\n",
      " [30, 11, 346, 259, 30, 904, 1404, 4, 3074, 1158, 1029, 51, 1336, 2324, 80, 22, 695, 70, 324, 2829, 248, 25, 5152, 2386]\n",
      "\n",
      "PADDED TOKEN SEQUENCES ::\n",
      "Dimensions of Training Data :: (20000, 50)\n",
      "Dimensions of Testing Data :: (2000, 50)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Map text data to integer values\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "MAX_NUM_TOKENS = train_data['class'].shape[0]  # set max to 20k\n",
    "\n",
    "tokenizer = Tokenizer(num_words = MAX_NUM_TOKENS)\n",
    "tokenizer.fit_on_texts(train_data['text'])\n",
    "print('\\nFound %s unique tokens' % len(tokenizer.word_index))\n",
    "print('Number of Tokens used ::', tokenizer.num_words)\n",
    "print('Longest Text Sample ::', len(train_data['text'].max()))\n",
    "\n",
    "# Vectorize text values\n",
    "sequences_train = tokenizer.texts_to_sequences(train_data['text'])\n",
    "sequences_test = tokenizer.texts_to_sequences(test_data['text'])\n",
    "print(\"\\nImported Sample Text ::\\n\", train_data['text'][pick_me])\n",
    "print(\"\\nTokenized Sample Text ::\\n\", sequences_train[pick_me])\n",
    "\n",
    "# Pad the token sequences to have equal length\n",
    "MAX_SENTENCE_LENGTH = 50  # set longest sequence of one token to 50 <<-- our pick\n",
    "x_train = pad_sequences(sequences_train, maxlen=MAX_SENTENCE_LENGTH)\n",
    "x_test = pad_sequences(sequences_test, maxlen=MAX_SENTENCE_LENGTH)\n",
    "print('\\nPADDED TOKEN SEQUENCES ::')\n",
    "print('Dimensions of Training Data :: ' + str(x_train.shape))\n",
    "print('Dimensions of Testing Data :: ' + str(x_test.shape) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe821f5-2ae0-44cb-93d9-05cb8b6b52fe",
   "metadata": {},
   "source": [
    "#### Categorize the outputs (y-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bdd8e69-2cad-4c70-a861-0a8f02cdba64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BEFORE CATEGORIZATION:\n",
      "Training Label Values :: [1 2]\n",
      "Testing Label Values :: [1 2]\n",
      "Training Label Shape :: (20000,)\n",
      "Testing Label Shape :: (2000,)\n",
      "\n",
      "AFTER CATEGORIZATION:\n",
      "Training Label Values :: [0 1]\n",
      "Testing Label Values :: [0 1]\n",
      "Training Label Shape :: (20000,)\n",
      "Testing Label Shape :: (2000,) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transform float Y-array values to integers\n",
    "from keras.utils import np_utils\n",
    "y_train = np.array(train_data['class'])\n",
    "y_test = np.array(test_data['class'])\n",
    "print('\\nBEFORE CATEGORIZATION:')\n",
    "print('Training Label Values ::', np.unique(y_train))\n",
    "print('Testing Label Values ::', np.unique(y_test))\n",
    "print('Training Label Shape ::', y_train.shape)\n",
    "print('Testing Label Shape ::', y_test.shape)\n",
    "y_train -= min(y_train)\n",
    "y_test -= min(y_test)\n",
    "NB_CLASSES = int(len(np.unique(y_train))) # number of classes\n",
    "print('\\nAFTER CATEGORIZATION:')\n",
    "print('Training Label Values ::', np.unique(y_train))\n",
    "print('Testing Label Values ::', np.unique(y_test))\n",
    "print('Training Label Shape ::', y_train.shape)\n",
    "print('Testing Label Shape ::', y_test.shape, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dd1bd1-1f37-4627-8a1e-59cf4724099f",
   "metadata": {},
   "source": [
    "#### Tokenize the inputs (x-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17936ef1-316e-4ad4-b988-b6cd4bbfabfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quantity of Dictionary Vectors :: 400000\n",
      "\n",
      "Sample Dictionary Word Vector :: \"the\" :: \n",
      " [-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\n",
      "  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\n",
      " -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\n",
      " -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\n",
      " -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\n",
      "  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\n",
      "  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\n",
      " -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\n",
      "  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\n",
      "  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\n",
      "  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\n",
      " -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\n",
      " -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\n",
      " -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\n",
      "  0.8278    0.27062 ] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the Tokenizer Dictionary database\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B/glove.6B.100d.txt', encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('\\nQuantity of Dictionary Vectors ::', len(embeddings_index))\n",
    "print('\\nSample Dictionary Word Vector :: \"the\" :: \\n', embeddings_index['the'], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c409d2e-5976-41f1-a139-68aa6e3d034e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding Matrix Shape :: (20000, 100)\n",
      "\n",
      "Word mapped to integer index = 1 is:  ['the']\n",
      "\n",
      " ['the']  is mapped to word vector\n",
      " [-0.038194   -0.24487001  0.72812003 -0.39961001  0.083172    0.043953\n",
      " -0.39140999  0.3344     -0.57545     0.087459    0.28786999 -0.06731\n",
      "  0.30906001 -0.26383999 -0.13231    -0.20757     0.33395001 -0.33848\n",
      " -0.31742999 -0.48335999  0.1464     -0.37303999  0.34577     0.052041\n",
      "  0.44946    -0.46970999  0.02628    -0.54154998 -0.15518001 -0.14106999\n",
      " -0.039722    0.28277001  0.14393     0.23464    -0.31020999  0.086173\n",
      "  0.20397     0.52623999  0.17163999 -0.082378   -0.71787    -0.41531\n",
      "  0.20334999 -0.12763     0.41367     0.55186999  0.57907999 -0.33476999\n",
      " -0.36559001 -0.54856998 -0.062892    0.26583999  0.30204999  0.99774998\n",
      " -0.80480999 -3.0243001   0.01254    -0.36941999  2.21670008  0.72201002\n",
      " -0.24978     0.92136002  0.034514    0.46744999  1.10790002 -0.19358\n",
      " -0.074575    0.23353    -0.052062   -0.22044     0.057162   -0.15806\n",
      " -0.30798    -0.41624999  0.37972     0.15006    -0.53211999 -0.20550001\n",
      " -1.25259995  0.071624    0.70564997  0.49744001 -0.42063001  0.26148\n",
      " -1.53799999 -0.30223    -0.073438   -0.28312001  0.37103999 -0.25217\n",
      "  0.016215   -0.017099   -0.38984001  0.87423998 -0.72569001 -0.51058\n",
      " -0.52028    -0.1459      0.82779998  0.27061999]\n",
      "\n",
      "Word mapped to integer index = 3785 is:  ['americanized']\n",
      "\n",
      " ['americanized']   is mapped to word vector\n",
      " [-0.66947001  0.53723001  0.39745     0.32043001 -0.41486001  0.49803999\n",
      "  0.19704001  0.24448    -0.62134999  0.57857001 -0.22586    -0.40845001\n",
      "  0.40952    -0.34717     0.50044     0.050629   -0.375      -0.39754\n",
      "  0.38991001  0.25169     0.021587   -0.24225    -0.34998    -0.14940999\n",
      "  0.30454999 -0.23317    -0.14366999 -0.53060001 -0.55974001 -0.092974\n",
      " -0.046677    0.53274     0.33741    -0.56555998  0.34239     0.49443999\n",
      "  0.21799    -0.13873     0.74792999 -0.23932     0.36660999 -0.45471999\n",
      " -0.1228      0.51141     0.046354    0.27860001  0.51424998 -0.10035\n",
      "  0.025134   -0.37825     0.52970999 -0.43452999  0.31461     0.052292\n",
      "  0.033863   -1.55299997 -0.17129999 -0.15787999  1.31739998  0.022283\n",
      " -0.015181    0.89787     0.11366    -0.20466     0.93682998  0.047078\n",
      "  0.19273999 -0.79610002  0.28378999 -0.29776001 -0.45321    -0.023545\n",
      "  0.14564     0.50563002  0.41980001  0.061762   -0.16913    -0.34931999\n",
      " -0.16711     0.037454    0.20475    -0.12714    -0.30002999 -0.30919999\n",
      " -1.2076     -0.023519   -0.21247999 -0.43500999  0.18122999 -0.11115\n",
      "  0.23374     0.063624    0.21408001 -0.10668    -0.28957999 -0.29569\n",
      " -0.76599002 -0.70016998  0.77849001  0.097057  ] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Map vectorized data to Tokenizer Dictionary: embedding_matrix\n",
    "EMBEDDING_DIM = len(embeddings_index['the'])  # length of each dictionary vector:: 100\n",
    "embedding_matrix = np.zeros((MAX_NUM_TOKENS, EMBEDDING_DIM))  # placeholder matrix\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > MAX_NUM_TOKENS - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "print('\\nEmbedding Matrix Shape ::', embedding_matrix.shape)\n",
    "\n",
    "# Report sample value of tokenized vector for 'the' & the random sample\n",
    "sample_word = [key for (key, value) in tokenizer.word_index.items() if value == 1]\n",
    "print('\\nWord mapped to integer index = 1 is: ', sample_word)\n",
    "print('\\n', sample_word,' is mapped to word vector\\n', embedding_matrix[1,:])\n",
    "sample_word = [key for (key, value) in tokenizer.word_index.items() if value == pick_me]\n",
    "print('\\nWord mapped to integer index =', pick_me, 'is: ', sample_word)\n",
    "print('\\n', sample_word,'  is mapped to word vector\\n', embedding_matrix[1000,:], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3ea4dd2-c778-43bd-9d5a-cbb2eeddd738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Shape  :: 2 dimensions (20000, 50)\n",
      "Training Labels :: 1 dimensions (20000,)\n",
      "\n",
      "Testing Shape   :: 2 dimensions (2000, 50)\n",
      "Testing Labels  :: 1 dimensions (2000,)\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "\n",
      "Imported Data\n",
      "Classes are::\t [0 1]\n",
      " # per class:\t [10002  9998]\n",
      "\n",
      " <Label>\t<Descriptor>\n",
      "    0\t\tnegative  \n",
      "    1\t\tpositive  \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "\n",
      "Sample text sequence (vectorized) ::\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,   30,   11,  346,  259,   30,  904, 1404,\n",
       "          4, 3074, 1158, 1029,   51, 1336, 2324,   80,   22,  695,   70,\n",
       "        324, 2829,  248,   25, 5152, 2386])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Describe the data shapes & labels\n",
    "print('\\nTraining Shape  ::', x_train.ndim, 'dimensions', x_train.shape)\n",
    "print('Training Labels ::', y_train.ndim, 'dimensions', y_train.shape)\n",
    "print('\\nTesting Shape   ::', x_test.ndim, 'dimensions', x_test.shape)\n",
    "print('Testing Labels  ::', y_test.ndim, 'dimensions', y_test.shape)\n",
    "print('- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -')\n",
    "\n",
    "# Peek at the data & labels\n",
    "print('\\nImported Data')\n",
    "print('Classes are::\\t', np.unique(y_train, return_counts=True)[0])\n",
    "print(' # per class:\\t', np.unique(y_train, return_counts=True)[1])\n",
    "\n",
    "# Describe the classifiers\n",
    "label_descriptors = ['negative', 'positive']\n",
    "print(f'\\n <Label>\\t<Descriptor>')\n",
    "for _ in range(len(np.unique(y_train))):\n",
    "    print(f'{np.unique(y_train)[_]:>5}\\t\\t{label_descriptors[_]:<10}')\n",
    "print('- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -')\n",
    "\n",
    "# Display the sample text as Tokenized Tensor\n",
    "print('\\nSample text sequence (vectorized) ::')\n",
    "display(x_train[pick_me])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7baa57-18c8-48a3-98ec-802a9a0224d7",
   "metadata": {},
   "source": [
    "#### Store pre-processed data in `h5` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "507caf38-06ed-4d62-9e7c-291d8cef22ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store tensor data in 'h5' file for later use\n",
    "import h5py\n",
    "with h5py.File('generated_files/Final_Hinrichs_source2-data.h5', 'w') as hf:\n",
    "    dataset_group = hf.create_group('dataset')\n",
    "    dataset_group.create_dataset('x_train', data=x_train)\n",
    "    dataset_group.create_dataset('x_test', data=x_test)\n",
    "    dataset_group.create_dataset('y_train', data=y_train)\n",
    "    dataset_group.create_dataset('y_test', data=y_test)\n",
    "    hf.create_dataset('embedding_matrix', data=embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272f26bc-c368-4247-80dc-aca9fe78fd89",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "<center><h3>Model Construction</h3></center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe7b046-b282-40aa-a5c7-c95e95887bb3",
   "metadata": {},
   "source": [
    "#### Import Source Data & Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "103f7744-b03d-46fd-a70d-b5447ff2f5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Shape :: (20000, 50)\n",
      "Training Labels :: (20000,)\n",
      "\n",
      "Testing Shape :: (2000, 50)\n",
      "Testing Labels :: (2000,)\n",
      "\n",
      "Dictionary Shape :: (20000, 100)\n",
      "\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
     ]
    }
   ],
   "source": [
    "# Import pre-processed tensor data from 'h5' file\n",
    "import numpy as np\n",
    "import h5py\n",
    "with h5py.File('generated_files/Final_Hinrichs_source2-data.h5', 'r') as hf:\n",
    "    x_train = np.array(hf['dataset/x_train'])\n",
    "    x_test = np.array(hf['dataset/x_test'])\n",
    "    y_train = np.array(hf['dataset/y_train'])\n",
    "    y_test = np.array(hf['dataset/y_test'])\n",
    "    embedding_matrix = np.array(hf['embedding_matrix'])\n",
    "\n",
    "# Describe the data shapes (4D Tensors)\n",
    "print('\\nTraining Shape ::', x_train.shape)\n",
    "print('Training Labels ::', y_train.shape)\n",
    "print('\\nTesting Shape ::', x_test.shape)\n",
    "print('Testing Labels ::', y_test.shape)\n",
    "print('\\nDictionary Shape ::', embedding_matrix.shape)\n",
    "\n",
    "# Import ML libraries & dependencies\n",
    "from tensorflow.keras.models import load_model, Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, Activation, Dropout\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import History, EarlyStopping, ModelCheckpoint\n",
    "print('\\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -')\n",
    "import time  # to track execution time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f15f18a-d84f-4f58-9e8a-b6516c1a446c",
   "metadata": {},
   "source": [
    "#### Define Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b6ac5d1-c95c-4d3d-89f4-e75f22e3f896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define fixed parameters for model fitting\n",
    "NB_CLASSES = int(len(set(y_train))) # number of classes:: 2\n",
    "N_SAMPLES = x_train.shape[0] # number of training samples:: 20k\n",
    "TEXT_DIM = (None, x_train.shape[1]) # shape of each sequence:: 1x50\n",
    "NB_EPOCH = 3*x_train.shape[1] # number of batch epoch:: 150 (we may adjust later)\n",
    "filepaths = list()  # to store list of h5 files containing fitted models\n",
    "\n",
    "# Define embedding layer using 'GLOVE' Dictionary Tokenizer\n",
    "MAX_NUM_TOKENS = embedding_matrix.shape[0] # number of training samples:: 20k\n",
    "EMBEDDING_DIM = embedding_matrix.shape[1]  # length of each dictionary vector:: 100\n",
    "MAX_SENTENCE_LENGTH = x_train.shape[1]  # longest sequence of one token:: 50\n",
    "embedding_layer = Embedding(MAX_NUM_TOKENS,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENTENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b33689-6226-4102-ab6e-8428846c21ba",
   "metadata": {},
   "source": [
    "#### Define Evaluative Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60287462-04a6-4148-95e5-b2fd477eda70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for plotting model performance\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "def plotHistory(tuned_model):\n",
    "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "    # Show Training Loss vs Validation Loss (to minimize)\n",
    "    axs[0].plot(tuned_model.history['loss'], color='magenta')\n",
    "    axs[0].plot(tuned_model.history['val_loss'], color='orange')\n",
    "    axs[0].set_title('loss vs epoch')\n",
    "    #axs[0].set_ylim(0.0,0.7)\n",
    "    axs[0].set_ylabel('loss')\n",
    "    axs[0].set_xlabel('epoch')\n",
    "    axs[0].legend(['training', 'validation'], loc='upper left')\n",
    "\n",
    "    # Show Training Accuracy vs Validation Accuracy (to maximize)\n",
    "    axs[1].plot(tuned_model.history['accuracy'], color='magenta')\n",
    "    axs[1].plot(tuned_model.history['val_accuracy'], color='orange')\n",
    "    axs[1].set_title('accuracy vs epoch')\n",
    "    axs[1].set_ylabel('accuracy')\n",
    "    axs[1].set_xlabel('epoch')\n",
    "    #axs[1].set_ylim(0.5,1)\n",
    "    axs[1].legend(['training', 'validation'], loc='upper left')\n",
    "    plt.show(block = False)\n",
    "    plt.show()\n",
    "\n",
    "# Define function to report best model performance from trials\n",
    "def getBestModelfromTrials(trials, modelname):\n",
    "    # Extracts all valid iterations of the hyperoptimization \n",
    "    valid_trial_list = [trial for trial in trials\n",
    "                            if STATUS_OK == trial['result']['status']]\n",
    "    \n",
    "    # Extracts obj. function value in all valid iterations of the hyperoptimization \n",
    "    losses = [float(trial['result']['loss']) for trial in valid_trial_list]\n",
    "    \n",
    "    # Finds the model with the lowest obj. function\n",
    "    index_having_minumum_loss = np.argmin(losses)\n",
    "    best_trial_obj = valid_trial_list[index_having_minumum_loss]\n",
    "    \n",
    "    # Extracts the model corresponding to the lowest obj. function\n",
    "    bestest_model_ever = best_trial_obj['result']['Trained_Model']\n",
    "    modelname = bestest_model_ever._name\n",
    "    filepath = 'generated_files/saved_bests/Final_Hinrichs_'+str(modelname)+'.json'\n",
    "    model_json = bestest_model_ever.to_json()\n",
    "    with open(filepath, 'w') as json_file:\n",
    "         json_file.write(model_json)\n",
    "    filepath = 'generated_files/saved_bests/Final_Hinrichs_'+str(modelname)+'_weights.h5'\n",
    "    bestest_model_ever.save_weights(filepath)\n",
    "    return best_trial_obj['result']['Trained_Model']\n",
    "    \n",
    "# Define function to report model accuracy separately\n",
    "from sklearn.metrics import accuracy_score\n",
    "def GetAccuracy(model_name, test_data, test_labels):\n",
    "    pred_prob = model_name.predict(test_data) # predict probabilities\n",
    "    pred_labels = np.where(pred_prob > 0.5, 1,0) # predicted class labels\n",
    "    true_labels = np.where(test_labels > 0.5, 1,0) # convert categoricalâ†’class labels\n",
    "    return accuracy_score(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb12037-7f22-4b0d-9afd-a7338f55c040",
   "metadata": {},
   "source": [
    "<u><center><h2>Model Architecture</h2></center></u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76578f1-9232-4fe3-a0ab-e592a57249b8",
   "metadata": {},
   "source": [
    "#### Define Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b271e5d7-4c51-456c-9c9e-aa96454c335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to instantiate an RNN Combination model\n",
    "COUNT = int(0)  # to track model build calls\n",
    "def model_maker(params, x_data, y_data):\n",
    "    global COUNT\n",
    "    COUNT += 1\n",
    "    modelname = 'hyp_opt_model_'+str(COUNT)\n",
    "    ## ---->> INSTANTIATE THE MODEL\n",
    "    start_clock = time.process_time_ns()\n",
    "    # Build a new Neural Network Model using RNN architectures\n",
    "    model = Sequential(name = modelname)\n",
    "    # ---- input layer :: Embedding Layer\n",
    "    model.add(embedding_layer)\n",
    "    # # -- layer LSTM #1\n",
    "    # model.add(LSTM(units = params['num_kernel'],\n",
    "    #                dropout = params['dropout'],\n",
    "    #                recurrent_dropout = params['dropout'],\n",
    "    #                return_sequences = True,  # train on multiple time points\n",
    "    #                name = 'LSTM_Layer_1'))\n",
    "    # # -- layer CNN :: got much better results after dropping this layer\n",
    "    # model.add(Conv1D(filters = params['num_kernel'], #!-can't be variable if first\n",
    "    #                  kernel_size = params['kernel_size'],\n",
    "    #                  activation = params['activation_function'],\n",
    "    #                  name = 'CNN_Layer'))\n",
    "    # # ---- first MaxPool layer\n",
    "    # model.add(MaxPooling1D(pool_size = params['size_pooling'],\n",
    "    #                        strides = params['strides']))\n",
    "    # model.add(Dropout(rate = params['dropout']))\n",
    "    # -- layer Bidirectional LSTM\n",
    "    model.add(Bidirectional(LSTM(units = int(params['num_kernel']/2),\n",
    "                                 dropout = params['dropout'],\n",
    "                                 recurrent_dropout = params['dropout'],\n",
    "                                 return_sequences = True,\n",
    "                                 name = 'Bidirectional_LSTM_Layer'),\n",
    "                            merge_mode = 'concat'))\n",
    "    # -- layer LSTM #2\n",
    "    model.add(LSTM(units = params['num_kernel'],\n",
    "                   dropout = params['dropout'],\n",
    "                   recurrent_dropout = params['dropout'],\n",
    "                   return_sequences = False,  # train on multiple time points\n",
    "                   name = 'LSTM_Layer_2'))\n",
    "    # -- output layer (Dense)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units = 1,\n",
    "                    activation = params['activation_function_output'],\n",
    "                    name = 'Output_Layer'))\n",
    "\n",
    "    ## ---->> COMPILE THE MODEL\n",
    "    model.compile(optimizer = params['optimizer'], \n",
    "                  loss = params['loss'], \n",
    "                  metrics = params['metrics'])\n",
    "    print(f'Success! {modelname} has been compiled!')\n",
    "\n",
    "    ## ---->> FIT THE MODEL\n",
    "    # Functions to stop overfitting\n",
    "    early_stopping_monitor = EarlyStopping(monitor='val_loss',\n",
    "                                           patience=params['patience'],\n",
    "                                           mode='min')\n",
    "    filepath = 'generated_files/Final_Hinrichs_checkpoint2.h5' # for tracking\n",
    "    checkpoint = ModelCheckpoint(filepath,\n",
    "                                 monitor='val_loss',\n",
    "                                 verbose= params['verbose'],\n",
    "                                 save_best_only=True)\n",
    "\n",
    "    # Fit a compiled model with the datasets presented\n",
    "    tuned_model = model.fit(x_data, y_data,\n",
    "                            batch_size = params['batch_size'],\n",
    "                            epochs = params['epochs'], \n",
    "                            verbose = params['verbose'],\n",
    "                            validation_split = params['validation_split'],\n",
    "                            callbacks = [checkpoint, early_stopping_monitor])\n",
    "\n",
    "    # Track the model's validation loss history\n",
    "    keys = tuned_model.history.keys()\n",
    "    res = [i for i in keys if ('val' in i and 'loss' in i)]\n",
    "    val_loss = min(tuned_model.history[res[0]])\n",
    "    print(f'Success! {str(model._name)} has been tuned!')\n",
    "    end_clock = time.process_time_ns()\n",
    "    print(f'Execution clocked at {(end_clock-start_clock)*10**(-9)} secs\\n')\n",
    "    return {'loss': val_loss, 'status': STATUS_OK, 'Trained_Model': model}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9415145c-087d-45cc-9228-b2916e5c50a5",
   "metadata": {},
   "source": [
    "#### Define Hyperparameter Optimization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c813dad-d104-4bdc-bbcd-af702ee0c952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Hyperparameter Optimization Function\n",
    "from hyperopt import fmin, tpe, STATUS_OK, Trials\n",
    "def optimize_model(function, x_data, y_data, params, Build=True):\n",
    "    # Run fmin to determine lowest loss using model parameters\n",
    "    trials = Trials()\n",
    "    best_model = fmin(partial(function, # fmin to minimize loss\n",
    "                              x_data = x_data,\n",
    "                              y_data = y_data),\n",
    "                      space = params, # hyperparameter space\n",
    "                      algo = tpe.suggest, # bayesian algorithm to be used\n",
    "                      max_evals = params['max_evals_build'] if (Build==True) else params['max_evals_train'],\n",
    "                      trials = trials)  # to save output\n",
    "    \n",
    "    # extracts all valid iterations of the hyperoptimization \n",
    "    valid_trial_list = [trial for trial in trials\n",
    "                            if STATUS_OK == trial['result']['status']]\n",
    "    \n",
    "    # extracts obj. function value in all valid iterations of the hyperoptimization \n",
    "    losses = [float(trial['result']['loss']) for trial in valid_trial_list]\n",
    "    \n",
    "    # find the one with lowest obj. function\n",
    "    index_having_minumum_loss = np.argmin(losses)\n",
    "    best_trial_obj = valid_trial_list[index_having_minumum_loss]\n",
    "    \n",
    "    # extracts the model corresponding to the lowest obj. function\n",
    "    bestest_model = best_trial_obj['result']['Trained_Model']\n",
    "    modelname = bestest_model._name\n",
    "    filepath = 'generated_files/saved_bests/Final_Hinrichs_'+str(modelname)+'.json'\n",
    "    model_json = bestest_model.to_json()\n",
    "    with open(filepath, 'w') as json_file:\n",
    "         json_file.write(model_json)\n",
    "    filepath = 'generated_files/saved_bests/Final_Hinrichs_'+str(modelname)+'_weights.h5'\n",
    "    bestest_model.save_weights(filepath)\n",
    "    \n",
    "    # Return trials object\n",
    "    least_loss = best_trial_obj['result']['loss']\n",
    "    print(f'Success! {str(modelname)} has been optimized!' \\\n",
    "          f'Lowest loss {least_loss:.8f}\\n')\n",
    "    return best_trial_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90942ed-a5a4-4f50-9411-fc3d7f707039",
   "metadata": {},
   "source": [
    "<u><center><h2>Model Training</h2></center></u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a018506f-ad82-4b41-a8eb-9a47a8d7ab6d",
   "metadata": {},
   "source": [
    "We attempted a number of trial models using our data with minimally-iterating, fixed hyperparameter variables in order to find the best-responding architecture to our data. Model architectures we attempted were: \n",
    "- Conv1D + MaxPool + Dropout â†’ BiDirectional LSTM â†’ LSTM â†’ Dense\n",
    "- LSTM â†’ BiDirectional LSTM â†’ LSTM â†’ Dense\n",
    "- BiDirectional LSTM â†’ Dense\n",
    "- BiDirectional LSTM â†’ LSTM â†’ Dense**\n",
    "\n",
    "**Our chosen architecture, based upon the output's lowest loss (~ 24%) & highest accuracy (~ 90%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3a3aa1-c20e-4680-a1b4-bf02b94d2901",
   "metadata": {},
   "source": [
    "#### Define Hyperparameter Tuning Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d366c81e-27f1-43b3-af57-1c0bddab74f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameter Domain Space to adjust per param in order of output\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll.base import scope\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from functools import partial\n",
    "setup_space = {\n",
    "    'activation_function': 'relu',\n",
    "    'activation_function_output': 'sigmoid',\n",
    "    'batch_size': int(10),\n",
    "    'dropout': float(.2),\n",
    "    'epochs': int(100),\n",
    "    'initializer': 'uniform',\n",
    "    'kernel_size': int(5),\n",
    "    'learning_rate' : float(.01),\n",
    "    'loss': 'binary_crossentropy',\n",
    "    'max_evals_build': int(1),  # for moderating execution time\n",
    "    'max_evals_train': int(10),  # during optimization\n",
    "    'metrics': 'accuracy',\n",
    "    'num_kernel': int(64),\n",
    "    'optimizer': 'adam',\n",
    "    'patience': int(2),\n",
    "    'size_pooling': int(4),\n",
    "    'strides': int(2),\n",
    "    'validation_split': float(.2),\n",
    "    'verbose': int(0)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f189712-4b95-4ae8-8253-df3728ba4104",
   "metadata": {},
   "source": [
    "#### Describe Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c447898e-6b66-45f5-a381-067fcd7ff97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"hyp_opt_model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 50, 100)           2000000   \n",
      "                                                                 \n",
      " bidirectional_7 (Bidirectio  (None, 50, 64)           34048     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " LSTM_Layer_2 (LSTM)         (None, 64)                33024     \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 64)                0         \n",
      "                                                                 \n",
      " Output_Layer (Dense)        (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,067,137\n",
      "Trainable params: 67,137\n",
      "Non-trainable params: 2,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Describe our chosen model architecture\n",
    "model_opt['result']['Trained_Model'].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7ca5429-2f49-462b-a383-2344c4a42073",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! hyp_opt_model_3 has been compiled!          \n",
      "Success! hyp_opt_model_3 has been tuned!             \n",
      "Execution clocked at 729.34375 secs                  \n",
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [14:52<00:00, 892.27s/trial, best loss: 0.31043487787246704]\n",
      "Success! hyp_opt_model_3 has been optimized!\n",
      "\tHere are the results:\n",
      "\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "# Compile & fit the model using hyperparameter tuning\n",
    "model_opt = optimize_model(model_maker, x_train, y_train, setup_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aebb5925-ca64-4600-8316-10a5a790a7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Our model hyp_opt_model_3 achieved minimized loss at 0.31043488\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate results\n",
    "best_model = model_opt['result']['Trained_Model']\n",
    "best_loss = model_opt['result']['loss']\n",
    "print(f'\\nOur model {best_model._name} achieved minimized', \\\n",
    "      f'loss at {best_loss:.8f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572bef69-af8a-4b0f-aafc-0d9bf289e7c3",
   "metadata": {},
   "source": [
    "<center><h2><u>Model Optimization</u></h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5be9004-5ee0-46e2-85e9-2fb0380aad49",
   "metadata": {},
   "source": [
    "#### Define Hyperparameter Optimization Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fcd170a5-4e13-49a6-8f22-b538a0647b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameter Variable Domain Space\n",
    "opt_space = {\n",
    "    'activation_function': hp.choice('activation_function',\n",
    "                                     ['relu','tanh','sigmoid']),\n",
    "    'activation_function_output': 'sigmoid',\n",
    "    'batch_size': scope.int(hp.quniform('batch_size',16,128,16)),\n",
    "    'dropout': hp.uniform('dropout',.20,.35),\n",
    "    'epochs': scope.int(hp.quniform('epochs',1,100,1)),\n",
    "    'initializer': 'uniform',\n",
    "    'kernel_size': scope.int(hp.quniform('kernel_size',2,5,1)),\n",
    "    'learning_rate' : hp.loguniform('learning_rate', np.log(0.005), np.log(0.1)),\n",
    "    'loss': 'binary_crossentropy',\n",
    "    'max_evals_build': int(1),  # for moderating execution time\n",
    "    'max_evals_train': int(10),  # max limit iterations of model fitting epochs\n",
    "    'metrics': 'accuracy',\n",
    "    'num_kernel': scope.int(hp.quniform('num_kernel',16,128,16)),\n",
    "    'optimizer': hp.choice('optimizer',['adadelta','adam','rmsprop']),\n",
    "    'patience': scope.int(hp.quniform('patience',2,8,1)),\n",
    "    'size_pooling': scope.int(hp.quniform('size_pooling',2,4,1)),\n",
    "    'strides': scope.int(hp.quniform('strides',1,2,1)),\n",
    "    'validation_split': float(.2),\n",
    "    'verbose': int(0)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12b5b09-4e4d-4da0-8366-1a65ca700b77",
   "metadata": {},
   "source": [
    "#### Define the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f531e5cc-c342-4bf8-bd45-42f436ab13fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! hyp_opt_model_4 has been compiled!          \n",
      "Success! hyp_opt_model_4 has been tuned!             \n",
      "Execution clocked at 1097.765625 secs                \n",
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [20:32<00:00, 1232.43s/trial, best loss: 0.6711501479148865]\n",
      "Success! hyp_opt_model_4 has been optimized!Lowest loss 0.67115015\n",
      "\n",
      "{'state': 2, 'tid': 0, 'spec': None, 'result': {'loss': 0.6711501479148865, 'status': 'ok', 'Trained_Model': <keras.engine.sequential.Sequential object at 0x0000026AD2BCA3A0>}, 'misc': {'tid': 0, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'activation_function': [0], 'batch_size': [0], 'dropout': [0], 'epochs': [0], 'kernel_size': [0], 'learning_rate': [0], 'num_kernel': [0], 'optimizer': [0], 'patience': [0], 'size_pooling': [0], 'strides': [0]}, 'vals': {'activation_function': [1], 'batch_size': [16.0], 'dropout': [0.33673238191258076], 'epochs': [21.0], 'kernel_size': [3.0], 'learning_rate': [0.014855186628828922], 'num_kernel': [80.0], 'optimizer': [0], 'patience': [7.0], 'size_pooling': [3.0], 'strides': [2.0]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2023, 7, 10, 4, 46, 46, 300000), 'refresh_time': datetime.datetime(2023, 7, 10, 5, 7, 18, 705000)}\n"
     ]
    }
   ],
   "source": [
    "# Now compile & fit the model using hyperparameter optimization variable space\n",
    "model_hyperopt = optimize_model(model_maker, x_train, y_train, opt_space)\n",
    "print(model_hyperopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dbd7ca1f-a86c-48c5-b2b4-274be6338d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Our model hyp_opt_model_4 achieved minimized loss at 0.67115015\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate results\n",
    "best_hyperopt_model = model_hyperopt['result']['Trained_Model']\n",
    "best_hyperopt_loss = model_hyperopt['result']['loss']\n",
    "print(f'\\nOur model {best_hyperopt_model._name} achieved minimized', \\\n",
    "      f'loss at {best_hyperopt_loss:.8f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf1b9b1-6aeb-4a29-a55d-970728df6abe",
   "metadata": {},
   "source": [
    "#### Optimize the Model (max_evals = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "77fec299-fd69-4060-8864-324ebef13a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! hyp_opt_model_5 has been compiled!           \n",
      "Success! hyp_opt_model_5 has been tuned!              \n",
      "Execution clocked at 1071.625 secs                    \n",
      "\n",
      "Success! hyp_opt_model_6 has been compiled!                                          \n",
      "Success! hyp_opt_model_6 has been tuned!                                             \n",
      "Execution clocked at 280.578125 secs                                                 \n",
      "\n",
      "Success! hyp_opt_model_7 has been compiled!                                          \n",
      "Success! hyp_opt_model_7 has been tuned!                                             \n",
      "Execution clocked at 1025.046875 secs                                                  \n",
      "\n",
      "Success! hyp_opt_model_8 has been compiled!                                            \n",
      "Success! hyp_opt_model_8 has been tuned!                                              \n",
      "Execution clocked at 1099.921875 secs                                                 \n",
      "\n",
      "Success! hyp_opt_model_9 has been compiled!                                           \n",
      "Success! hyp_opt_model_9 has been tuned!                                              \n",
      "Execution clocked at 2101.3125 secs                                                   \n",
      "\n",
      "Success! hyp_opt_model_10 has been compiled!                                          \n",
      "Success! hyp_opt_model_10 has been tuned!                                             \n",
      "Execution clocked at 1169.0 secs                                                      \n",
      "\n",
      "Success! hyp_opt_model_11 has been compiled!                                          \n",
      "Success! hyp_opt_model_11 has been tuned!                                          \n",
      "Execution clocked at 821.796875 secs                                               \n",
      "\n",
      "Success! hyp_opt_model_12 has been compiled!                                       \n",
      "Success! hyp_opt_model_12 has been tuned!                                          \n",
      "Execution clocked at 855.96875 secs                                                \n",
      "\n",
      "Success! hyp_opt_model_13 has been compiled!                                       \n",
      "Success! hyp_opt_model_13 has been tuned!                                          \n",
      "Execution clocked at 1452.265625 secs                                              \n",
      "\n",
      "Success! hyp_opt_model_14 has been compiled!                                        \n",
      "Success! hyp_opt_model_14 has been tuned!                                           \n",
      "Execution clocked at 372.5 secs                                                     \n",
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [3:17:14<00:00, 1183.48s/trial, best loss: 0.3067939877510071]\n",
      "Success! hyp_opt_model_7 has been optimized!Lowest loss 0.30679399\n",
      "\n",
      "{'state': 2, 'tid': 2, 'spec': None, 'result': {'loss': 0.3067939877510071, 'status': 'ok', 'Trained_Model': <keras.engine.sequential.Sequential object at 0x0000026AB7101DF0>}, 'misc': {'tid': 2, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'activation_function': [2], 'batch_size': [2], 'dropout': [2], 'epochs': [2], 'kernel_size': [2], 'learning_rate': [2], 'num_kernel': [2], 'optimizer': [2], 'patience': [2], 'size_pooling': [2], 'strides': [2]}, 'vals': {'activation_function': [1], 'batch_size': [112.0], 'dropout': [0.3274283406727577], 'epochs': [54.0], 'kernel_size': [3.0], 'learning_rate': [0.013220910276634638], 'num_kernel': [80.0], 'optimizer': [1], 'patience': [4.0], 'size_pooling': [2.0], 'strides': [2.0]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2023, 7, 10, 6, 3, 14, 707000), 'refresh_time': datetime.datetime(2023, 7, 10, 6, 22, 3, 869000)}\n"
     ]
    }
   ],
   "source": [
    "# Train the model using optimized parameters with higher iteration count\n",
    "model_hyperopt = optimize_model(model_maker, x_train, y_train, opt_space, Build=False)\n",
    "print(model_hyperopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f2bd9f72-06cf-4b96-8a6d-44711cf2587f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Our model hyp_opt_model_7 achieved minimized loss at 0.30679399\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate results\n",
    "bestest_hyperopt_model = model_hyperopt['result']['Trained_Model']\n",
    "bestest_hyperopt_loss = model_hyperopt['result']['loss']\n",
    "print(f'\\nOur model {bestest_hyperopt_model._name} achieved minimized', \\\n",
    "      f'loss at {bestest_hyperopt_loss:.8f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6540ffda-b4a8-4fd4-b981-2a5d4c7d7221",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "<center><h2>Model Performance Evaluation</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0c0653-900a-475e-b850-19442617fd47",
   "metadata": {},
   "source": [
    "Now that our model is tuned & optimized for least error, we can use it to get a real-time accuracy reading on our Testing Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b07cfef2-1368-43e9-8928-ff89539d74c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve saved models from saved trials\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.models import model_from_json\n",
    "best_models = []\n",
    "best_directory = Path('generated_files/saved_bests/')\n",
    "for ea, file in enumerate(best_directory.iterdir()):\n",
    "    if file.is_file and '.json' in file.name:\n",
    "        f = open(str(file),'r')\n",
    "        saved_best = f.read()\n",
    "        f.close()\n",
    "        saved_best = model_from_json(saved_best)\n",
    "    else:\n",
    "        saved_best.load_weights(file)\n",
    "    best_models.append(saved_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "254fd8d3-3200-4ae3-a322-ce0c2659b63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 12ms/step\n",
      "\n",
      "\thyp_opt_model_3.....\n",
      "Model Accuracy :: 0.8505\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "63/63 [==============================] - 1s 12ms/step\n",
      "\n",
      "\thyp_opt_model_3.....\n",
      "Model Accuracy :: 0.8505\n",
      "63/63 [==============================] - 1s 12ms/step\n",
      "\n",
      "\thyp_opt_model_4.....\n",
      "Model Accuracy :: 0.623\n",
      "63/63 [==============================] - 1s 12ms/step\n",
      "\n",
      "\thyp_opt_model_4.....\n",
      "Model Accuracy :: 0.623\n",
      "63/63 [==============================] - 1s 12ms/step\n",
      "\n",
      "\thyp_opt_model_7.....\n",
      "Model Accuracy :: 0.8575\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "63/63 [==============================] - 1s 12ms/step\n",
      "\n",
      "\thyp_opt_model_7.....\n",
      "Model Accuracy :: 0.8575\n",
      "\n",
      "The best-performing model on the Test Data is\n",
      "\t -->>  hyp_opt_model_7  <<-- \n",
      "with an accuracy score of 0.8575 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run prediction of each model against testing data\n",
    "# losses = [best_loss, best_hyperopt_loss, bestest_hyperopt_loss]\n",
    "# best_models = [best_model, best_hyperopt_model, bestest_hyperopt_model]\n",
    "best_loss, best_score = int(100), int(0)\n",
    "for i, model in enumerate(best_models):\n",
    "    # loss = losses[i]\n",
    "    score = GetAccuracy(model, x_test, y_test)\n",
    "    print('\\n\\t', str(model._name), '.....', sep='')\n",
    "    # print('Model Loss ::', loss)\n",
    "    print('Model Accuracy ::', score)\n",
    "    if score > best_score: # and loss < best_loss:\n",
    "        best_model_name = str(model._name)\n",
    "        # best_loss = loss\n",
    "        best_score = score\n",
    "        model.save('generated_files/Final_Hinrichs_best-overall.h5')\n",
    "print('\\nThe best-performing model on the Test Data is\\n\\t -->> ', best_model_name, \\\n",
    "      ' <<-- \\nwith an accuracy score of', best_score,'\\n') # ,'& loss of',best_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ef2d1f-6601-40f8-9ee4-1558b804d692",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "<center><h1>Analysis</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626a9d86-193e-418d-823a-1b32fd4570db",
   "metadata": {},
   "source": [
    "| Model Name | Loss Report | Accuracy |\n",
    "| --- | --- | --- |\n",
    "| hyp_opt_model_3 | .3104 | .8505 |\n",
    "| hyp_opt_model_4 | .6712 | .6230 |\n",
    "| hyp_opt_model_7 | .3070 | .8575 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ef84eb-3008-4019-9f4c-552e96d3d4a2",
   "metadata": {},
   "source": [
    "<h4>The best model is:</h4>\n",
    "`hyp_opt_model_7` with `max_eval = 10` giving the minimal loss of 30.7% using hyperparameter optimization.\n",
    "<h4>The best parameters are:</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc9a700-445f-44ab-bc42-2f18da9056ef",
   "metadata": {},
   "source": [
    "- 'activation_function': ['tanh']\n",
    "- 'batch_size': [112.0]\n",
    "- 'dropout': [0.3274283406727577]\n",
    "- 'epochs': [54.0]\n",
    "- 'kernel_size': [3.0]\n",
    "- 'learning_rate': [0.013220910276634638]\n",
    "- 'num_kernel': [80.0]\n",
    "- 'optimizer': ['adam']\n",
    "- 'patience': [4.0]\n",
    "- 'size_pooling': [2.0]\n",
    "- 'strides': [2.0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3a20f4-306d-401a-b2f6-83b9ff817fe4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
